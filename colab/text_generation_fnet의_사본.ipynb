{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jub97l6ZHLc"
      },
      "source": [
        "# Text Generation using FNet\n",
        "\n",
        "**Author:** [Darshan Deshpande](https://twitter.com/getdarshan)<br>\n",
        "**Date created:** 2021/10/05<br>\n",
        "**Last modified:** 2021/10/05<br>\n",
        "**Description:** FNet transformer for text generation in Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie6ibiHkZHLe"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The original transformer implementation (Vaswani et al., 2017) was one of the major\n",
        "breakthroughs in Natural Language Processing, giving rise to important architectures such BERT and GPT.\n",
        "However, the drawback of these architectures is\n",
        "that the self-attention mechanism they use is computationally expensive. The FNet\n",
        "architecture proposes to replace this self-attention attention with a leaner mechanism:\n",
        "a Fourier transformation-based linear mixer for input tokens.\n",
        "\n",
        "The FNet model was able to achieve 92-97% of BERT's accuracy while training 80% faster on\n",
        "GPUs and almost 70% faster on TPUs. This type of design provides an efficient and small\n",
        "model size, leading to faster inference times.\n",
        "\n",
        "In this example, we will implement and train this architecture on the Cornell Movie\n",
        "Dialog corpus to show the applicability of this model to text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTR7mDZGZHLe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mBaAQjmxZHLe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Defining hyperparameters\n",
        "\n",
        "VOCAB_SIZE = 8192\n",
        "MAX_SAMPLES = 50000\n",
        "BUFFER_SIZE = 20000\n",
        "MAX_LENGTH = 40\n",
        "EMBED_DIM = 256\n",
        "LATENT_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDrzkwcDZHLf"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "We will be using the Cornell Dialog Corpus. We will parse the movie conversations into\n",
        "questions and answers sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZuwEtehRZHLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2affdc9-87bc-43ed-bdb7-25841083a677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9920512/9916637 [==============================] - 0s 0us/step\n",
            "9928704/9916637 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_zip = keras.utils.get_file(\n",
        "    \"cornell_movie_dialogs.zip\",\n",
        "    origin=\"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "\n",
        "path_to_dataset = os.path.join(\n",
        "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\"\n",
        ")\n",
        "path_to_movie_lines = os.path.join(path_to_dataset, \"movie_lines.txt\")\n",
        "path_to_movie_conversations = os.path.join(path_to_dataset, \"movie_conversations.txt\")\n",
        "\n",
        "\n",
        "def load_conversations():\n",
        "    # Helper function for loading the conversation splits\n",
        "    id2line = {}\n",
        "    with open(path_to_movie_lines, errors=\"ignore\") as file:\n",
        "        lines = file.readlines()\n",
        "    for line in lines:\n",
        "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
        "        id2line[parts[0]] = parts[4]\n",
        "\n",
        "    inputs, outputs = [], []\n",
        "    with open(path_to_movie_conversations, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    for line in lines:\n",
        "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
        "        # get conversation in a list of line ID\n",
        "        conversation = [line[1:-1] for line in parts[3][1:-1].split(\", \")]\n",
        "        for i in range(len(conversation) - 1):\n",
        "            inputs.append(id2line[conversation[i]])\n",
        "            outputs.append(id2line[conversation[i + 1]])\n",
        "            if len(inputs) >= MAX_SAMPLES:\n",
        "                return inputs, outputs\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "questions, answers = load_conversations()\n",
        "\n",
        "# Splitting training and validation sets\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((questions[:40000], answers[:40000]))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((questions[40000:], answers[40000:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PflzVumtZHLg"
      },
      "source": [
        "### Preprocessing and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FHVPkH0VZHLg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_text(sentence):\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    # Adding a space between the punctuation and the last word to allow better tokenization\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"([?.!,])\", r\" \\1 \")\n",
        "    # Replacing multiple continuous spaces with a single space\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"\\s\\s+\", \" \")\n",
        "    # Replacing non english words with spaces\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"[^a-z?.!,]+\", \" \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    sentence = tf.strings.join([\"[start]\", sentence, \"[end]\"], separator=\" \")\n",
        "    return sentence\n",
        "\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    VOCAB_SIZE,\n",
        "    standardize=preprocess_text,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "# We will adapt the vectorizer to both the questions and answers\n",
        "# This dataset is batched to parallelize and speed up the process\n",
        "vectorizer.adapt(tf.data.Dataset.from_tensor_slices((questions + answers)).batch(128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjMrOIO7ZHLh"
      },
      "source": [
        "### Tokenizing and padding sentences using `TextVectorization`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_dmaWyCTZHLh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vectorize_text(inputs, outputs):\n",
        "    inputs, outputs = vectorizer(inputs), vectorizer(outputs)\n",
        "    # One extra padding token to the right to match the output shape\n",
        "    outputs = tf.pad(outputs, [[0, 1]])\n",
        "    return (\n",
        "        {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:-1]},\n",
        "        {\"outputs\": outputs[1:]},\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset.cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-VEGo-HZHLi"
      },
      "source": [
        "## Creating the FNet Encoder\n",
        "\n",
        "The FNet paper proposes a replacement for the standard attention mechanism used by the\n",
        "Transformer architecture (Vaswani et al., 2017).\n",
        "\n",
        "![Architecture](https://i.imgur.com/rLg47qU.png)\n",
        "\n",
        "The outputs of the FFT layer are complex numbers. To avoid dealing with complex layers,\n",
        "only the real part (the magnitude) is extracted.\n",
        "\n",
        "The dense layers that follow the Fourier transformation act as convolutions applied on\n",
        "the frequency domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O3pd36PoZHLi"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FNetEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
        "        super(FNetEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Casting the inputs to complex64\n",
        "        inp_complex = tf.cast(inputs, tf.complex64)\n",
        "        # Projecting the inputs to the frequency domain using FFT2D and\n",
        "        # extracting the real part of the output\n",
        "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
        "        proj_input = self.layernorm_1(inputs + fft)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_RM0hV9ZHLi"
      },
      "source": [
        "## Creating the Decoder\n",
        "\n",
        "The decoder architecture remains the same as the one proposed by (Vaswani et al., 2017)\n",
        "in the original transformer architecture, consisting of an embedding, positional\n",
        "encoding, two masked multihead attention layers and finally the dense output layers.\n",
        "The architecture that follows is taken from\n",
        "[Deep Learning with Python, second edition, chapter 11](https://www.manning.com/books/deep-learning-with-python-second-edition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kmJy_bYcZHLi"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class FNetDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(FNetDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
        "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(\n",
        "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
        "    )\n",
        "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
        "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    decoder_outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model(\n",
        "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
        "    )\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    fnet = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"fnet\")\n",
        "    return fnet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaARk6BIZHLj"
      },
      "source": [
        "## Creating and Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ujNwoUMBZHLj"
      },
      "outputs": [],
      "source": [
        "fnet = create_model()\n",
        "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"TextGenerationUsingFNet.h5\"\n",
        "callback_list = [\n",
        "             keras.callbacks.ModelCheckpoint(filename, monitor='accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')]"
      ],
      "metadata": {
        "id": "IWsw_umMZSWc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2aHIbhrZHLj"
      },
      "source": [
        "Here, the `epochs` parameter is set to a single epoch, but in practice the model will take around\n",
        "**20-30 epochs** of training to start outputting comprehensible sentences. Although accuracy\n",
        "is not a good measure for this task, we will use it just to get a hint of the improvement\n",
        "of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hwYYBsRTZHLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98d5a57-a1a8-457f-e729-921d561c6d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.6329 - accuracy: 0.2770\n",
            "Epoch 1: accuracy improved from -inf to 0.27696, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 54s 70ms/step - loss: 1.6329 - accuracy: 0.2770 - val_loss: 1.4490 - val_accuracy: 0.3168\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.4622 - accuracy: 0.3124\n",
            "Epoch 2: accuracy improved from 0.27696 to 0.31238, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.4622 - accuracy: 0.3124 - val_loss: 1.4139 - val_accuracy: 0.3276\n",
            "Epoch 3/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.4052 - accuracy: 0.3238\n",
            "Epoch 3: accuracy improved from 0.31238 to 0.32381, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 1.4052 - accuracy: 0.3238 - val_loss: 1.4035 - val_accuracy: 0.3303\n",
            "Epoch 4/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.3634 - accuracy: 0.3332\n",
            "Epoch 4: accuracy improved from 0.32381 to 0.33320, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 1.3634 - accuracy: 0.3332 - val_loss: 1.3948 - val_accuracy: 0.3351\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.3275 - accuracy: 0.3424\n",
            "Epoch 5: accuracy improved from 0.33320 to 0.34241, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 1.3275 - accuracy: 0.3424 - val_loss: 1.3959 - val_accuracy: 0.3335\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.2951 - accuracy: 0.3503\n",
            "Epoch 6: accuracy improved from 0.34241 to 0.35030, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 1.2951 - accuracy: 0.3503 - val_loss: 1.3958 - val_accuracy: 0.3351\n",
            "Epoch 7/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.2633 - accuracy: 0.3588\n",
            "Epoch 7: accuracy improved from 0.35030 to 0.35882, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 1.2633 - accuracy: 0.3588 - val_loss: 1.4052 - val_accuracy: 0.3324\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.2323 - accuracy: 0.3677\n",
            "Epoch 8: accuracy improved from 0.35882 to 0.36765, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 1.2323 - accuracy: 0.3677 - val_loss: 1.4166 - val_accuracy: 0.3318\n",
            "Epoch 9/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.2013 - accuracy: 0.3761\n",
            "Epoch 9: accuracy improved from 0.36765 to 0.37612, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 1.2013 - accuracy: 0.3761 - val_loss: 1.4358 - val_accuracy: 0.3261\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.1708 - accuracy: 0.3864\n",
            "Epoch 10: accuracy improved from 0.37612 to 0.38640, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 1.1708 - accuracy: 0.3864 - val_loss: 1.4542 - val_accuracy: 0.3281\n",
            "Epoch 11/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.1414 - accuracy: 0.3956\n",
            "Epoch 11: accuracy improved from 0.38640 to 0.39558, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 1.1414 - accuracy: 0.3956 - val_loss: 1.4777 - val_accuracy: 0.3229\n",
            "Epoch 12/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.1115 - accuracy: 0.4058\n",
            "Epoch 12: accuracy improved from 0.39558 to 0.40577, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 1.1115 - accuracy: 0.4058 - val_loss: 1.4899 - val_accuracy: 0.3225\n",
            "Epoch 13/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.0836 - accuracy: 0.4156\n",
            "Epoch 13: accuracy improved from 0.40577 to 0.41559, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 1.0836 - accuracy: 0.4156 - val_loss: 1.5219 - val_accuracy: 0.3164\n",
            "Epoch 14/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.0552 - accuracy: 0.4261\n",
            "Epoch 14: accuracy improved from 0.41559 to 0.42609, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 1.0552 - accuracy: 0.4261 - val_loss: 1.5379 - val_accuracy: 0.3173\n",
            "Epoch 15/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.0280 - accuracy: 0.4362\n",
            "Epoch 15: accuracy improved from 0.42609 to 0.43624, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 1.0280 - accuracy: 0.4362 - val_loss: 1.5636 - val_accuracy: 0.3149\n",
            "Epoch 16/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 1.0032 - accuracy: 0.4464\n",
            "Epoch 16: accuracy improved from 0.43624 to 0.44636, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 1.0032 - accuracy: 0.4464 - val_loss: 1.6082 - val_accuracy: 0.3140\n",
            "Epoch 17/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.9793 - accuracy: 0.4566\n",
            "Epoch 17: accuracy improved from 0.44636 to 0.45659, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.9793 - accuracy: 0.4566 - val_loss: 1.6245 - val_accuracy: 0.3066\n",
            "Epoch 18/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.9545 - accuracy: 0.4655\n",
            "Epoch 18: accuracy improved from 0.45659 to 0.46552, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.9545 - accuracy: 0.4655 - val_loss: 1.6637 - val_accuracy: 0.3066\n",
            "Epoch 19/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.9334 - accuracy: 0.4751\n",
            "Epoch 19: accuracy improved from 0.46552 to 0.47511, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.9334 - accuracy: 0.4751 - val_loss: 1.6727 - val_accuracy: 0.3003\n",
            "Epoch 20/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.9116 - accuracy: 0.4842\n",
            "Epoch 20: accuracy improved from 0.47511 to 0.48417, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.9116 - accuracy: 0.4842 - val_loss: 1.7059 - val_accuracy: 0.3033\n",
            "Epoch 21/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.8908 - accuracy: 0.4933\n",
            "Epoch 21: accuracy improved from 0.48417 to 0.49332, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.8908 - accuracy: 0.4933 - val_loss: 1.7522 - val_accuracy: 0.3006\n",
            "Epoch 22/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.8702 - accuracy: 0.5017\n",
            "Epoch 22: accuracy improved from 0.49332 to 0.50168, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.8702 - accuracy: 0.5017 - val_loss: 1.7951 - val_accuracy: 0.2962\n",
            "Epoch 23/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.8515 - accuracy: 0.5105\n",
            "Epoch 23: accuracy improved from 0.50168 to 0.51055, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.8515 - accuracy: 0.5105 - val_loss: 1.8003 - val_accuracy: 0.2935\n",
            "Epoch 24/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.8356 - accuracy: 0.5176\n",
            "Epoch 24: accuracy improved from 0.51055 to 0.51759, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.8356 - accuracy: 0.5176 - val_loss: 1.8264 - val_accuracy: 0.2907\n",
            "Epoch 25/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.5264\n",
            "Epoch 25: accuracy improved from 0.51759 to 0.52635, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.8167 - accuracy: 0.5264 - val_loss: 1.8522 - val_accuracy: 0.2901\n",
            "Epoch 26/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7994 - accuracy: 0.5344\n",
            "Epoch 26: accuracy improved from 0.52635 to 0.53441, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.7994 - accuracy: 0.5344 - val_loss: 1.8813 - val_accuracy: 0.2903\n",
            "Epoch 27/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7831 - accuracy: 0.5422\n",
            "Epoch 27: accuracy improved from 0.53441 to 0.54218, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.7831 - accuracy: 0.5422 - val_loss: 1.9340 - val_accuracy: 0.2845\n",
            "Epoch 28/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7696 - accuracy: 0.5486\n",
            "Epoch 28: accuracy improved from 0.54218 to 0.54860, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.7696 - accuracy: 0.5486 - val_loss: 1.9561 - val_accuracy: 0.2879\n",
            "Epoch 29/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7538 - accuracy: 0.5557\n",
            "Epoch 29: accuracy improved from 0.54860 to 0.55572, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.7538 - accuracy: 0.5557 - val_loss: 1.9871 - val_accuracy: 0.2826\n",
            "Epoch 30/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7409 - accuracy: 0.5622\n",
            "Epoch 30: accuracy improved from 0.55572 to 0.56225, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.7409 - accuracy: 0.5622 - val_loss: 2.0006 - val_accuracy: 0.2873\n",
            "Epoch 31/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7269 - accuracy: 0.5684\n",
            "Epoch 31: accuracy improved from 0.56225 to 0.56837, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.7269 - accuracy: 0.5684 - val_loss: 2.0377 - val_accuracy: 0.2819\n",
            "Epoch 32/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7147 - accuracy: 0.5744\n",
            "Epoch 32: accuracy improved from 0.56837 to 0.57437, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.7147 - accuracy: 0.5744 - val_loss: 2.0438 - val_accuracy: 0.2801\n",
            "Epoch 33/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.5810\n",
            "Epoch 33: accuracy improved from 0.57437 to 0.58103, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.7015 - accuracy: 0.5810 - val_loss: 2.0798 - val_accuracy: 0.2837\n",
            "Epoch 34/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.5874\n",
            "Epoch 34: accuracy improved from 0.58103 to 0.58743, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.6884 - accuracy: 0.5874 - val_loss: 2.1242 - val_accuracy: 0.2756\n",
            "Epoch 35/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6775 - accuracy: 0.5931\n",
            "Epoch 35: accuracy improved from 0.58743 to 0.59310, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.6775 - accuracy: 0.5931 - val_loss: 2.1113 - val_accuracy: 0.2777\n",
            "Epoch 36/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.5981\n",
            "Epoch 36: accuracy improved from 0.59310 to 0.59810, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.6676 - accuracy: 0.5981 - val_loss: 2.1821 - val_accuracy: 0.2781\n",
            "Epoch 37/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.6030\n",
            "Epoch 37: accuracy improved from 0.59810 to 0.60304, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.6579 - accuracy: 0.6030 - val_loss: 2.1687 - val_accuracy: 0.2764\n",
            "Epoch 38/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6452 - accuracy: 0.6091\n",
            "Epoch 38: accuracy improved from 0.60304 to 0.60913, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.6452 - accuracy: 0.6091 - val_loss: 2.2149 - val_accuracy: 0.2739\n",
            "Epoch 39/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.6145\n",
            "Epoch 39: accuracy improved from 0.60913 to 0.61455, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.6354 - accuracy: 0.6145 - val_loss: 2.2148 - val_accuracy: 0.2747\n",
            "Epoch 40/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6261 - accuracy: 0.6198\n",
            "Epoch 40: accuracy improved from 0.61455 to 0.61985, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.6261 - accuracy: 0.6198 - val_loss: 2.2331 - val_accuracy: 0.2719\n",
            "Epoch 41/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6145 - accuracy: 0.6253\n",
            "Epoch 41: accuracy improved from 0.61985 to 0.62532, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.6145 - accuracy: 0.6253 - val_loss: 2.3025 - val_accuracy: 0.2708\n",
            "Epoch 42/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.6281\n",
            "Epoch 42: accuracy improved from 0.62532 to 0.62806, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.6083 - accuracy: 0.6281 - val_loss: 2.3484 - val_accuracy: 0.2694\n",
            "Epoch 43/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5982 - accuracy: 0.6338\n",
            "Epoch 43: accuracy improved from 0.62806 to 0.63376, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.5982 - accuracy: 0.6338 - val_loss: 2.2998 - val_accuracy: 0.2704\n",
            "Epoch 44/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5901 - accuracy: 0.6380\n",
            "Epoch 44: accuracy improved from 0.63376 to 0.63805, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.5901 - accuracy: 0.6380 - val_loss: 2.3445 - val_accuracy: 0.2681\n",
            "Epoch 45/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.6432\n",
            "Epoch 45: accuracy improved from 0.63805 to 0.64323, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.5805 - accuracy: 0.6432 - val_loss: 2.3762 - val_accuracy: 0.2691\n",
            "Epoch 46/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5745 - accuracy: 0.6460\n",
            "Epoch 46: accuracy improved from 0.64323 to 0.64600, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 0.5745 - accuracy: 0.6460 - val_loss: 2.4277 - val_accuracy: 0.2662\n",
            "Epoch 47/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.6500\n",
            "Epoch 47: accuracy improved from 0.64600 to 0.65003, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.5676 - accuracy: 0.6500 - val_loss: 2.3749 - val_accuracy: 0.2667\n",
            "Epoch 48/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5584 - accuracy: 0.6548\n",
            "Epoch 48: accuracy improved from 0.65003 to 0.65475, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.5584 - accuracy: 0.6548 - val_loss: 2.4334 - val_accuracy: 0.2693\n",
            "Epoch 49/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.6591\n",
            "Epoch 49: accuracy improved from 0.65475 to 0.65912, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.5503 - accuracy: 0.6591 - val_loss: 2.4249 - val_accuracy: 0.2664\n",
            "Epoch 50/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.6639\n",
            "Epoch 50: accuracy improved from 0.65912 to 0.66391, saving model to TextGenerationUsingFNet.h5\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.5429 - accuracy: 0.6639 - val_loss: 2.4729 - val_accuracy: 0.2632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdff9fbbbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "fnet.fit(train_dataset, epochs=50, validation_data=val_dataset, callbacks=callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txni4YwH7ZXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIJ1cbXAZHLj"
      },
      "source": [
        "## Performing inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XfkDx_krZHLj"
      },
      "outputs": [],
      "source": [
        "VOCAB = vectorizer.get_vocabulary()\n",
        "\n",
        "\n",
        "def decode_sentence(input_sentence):\n",
        "    # Mapping the input sentence to tokens and adding start and end tokens\n",
        "    tokenized_input_sentence = vectorizer(\n",
        "        tf.constant(\"[start] \" + preprocess_text(input_sentence) + \" [end]\")\n",
        "    )\n",
        "    # Initializing the initial sentence consisting of only the start token.\n",
        "    tokenized_target_sentence = tf.expand_dims(VOCAB.index(\"[start]\"), 0)\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # Get the predictions\n",
        "        predictions = fnet.predict(\n",
        "            {\n",
        "                \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
        "                \"decoder_inputs\": tf.expand_dims(\n",
        "                    tf.pad(\n",
        "                        tokenized_target_sentence,\n",
        "                        [[0, MAX_LENGTH - tf.shape(tokenized_target_sentence)[0]]],\n",
        "                    ),\n",
        "                    0,\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "        # Calculating the token with maximum probability and getting the corresponding word\n",
        "        sampled_token_index = tf.argmax(predictions[0, i, :])\n",
        "        sampled_token = VOCAB[sampled_token_index.numpy()]\n",
        "        # If sampled token is the end token then stop generating and return the sentence\n",
        "        if tf.equal(sampled_token_index, VOCAB.index(\"[end]\")):\n",
        "            break\n",
        "        decoded_sentence += sampled_token + \" \"\n",
        "        tokenized_target_sentence = tf.concat(\n",
        "            [tokenized_target_sentence, [sampled_token_index]], 0\n",
        "        )\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7rOJVA0ZHLj"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This example shows how to train and perform inference using the FNet model.\n",
        "For getting insight into the architecture or for further reading, you can refer to:\n",
        "\n",
        "1. [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824v3)\n",
        "(Lee-Thorp et al., 2021)\n",
        "2. [Attention Is All You Need](https://arxiv.org/abs/1706.03762v5) (Vaswani et al.,\n",
        "2017)\n",
        "\n",
        "Thanks to Fran√ßois Chollet for his Keras example on\n",
        "[English-to-Spanish translation with a sequence-to-sequence Transformer](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)\n",
        "from which the decoder implementation was extracted."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sentence(\"Where have you been all this time?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "t685OVu0ZXC_",
        "outputId": "4388867a-8bf2-4af0-c471-e84dc4599f4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i m not sure . i m in jail and i didn t , eddie , you know . that s what you re right , eddie , was that . you know . and that s what you  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sentence(\"She okay?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GpF_g_tzZXzW",
        "outputId": "cdea1cab-91f0-470f-ce73-cef1e55dae7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i m sorry , i didn t mean to you . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sentence(\"Like my fear of wearing pastels?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OJnklaHRZYkN",
        "outputId": "f1bc67df-315e-4466-fd77-f39b04794ae4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you re right . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sentence(\"Is everything ready?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gd4NtV0zZba3",
        "outputId": "c30bc532-369b-4f99-dd25-e01f91dda953"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yeah , well , you got to get out . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sentence(\"Where have you been all this time?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Qu0KpANrZYgU",
        "outputId": "304876d8-e4ab-4caf-9883-3c00906ff39a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i m not sure . i m in jail and i didn t , eddie , you know . that s what you re right , eddie , was that . you know . and that s what you  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gW4wj7-r64Yl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}