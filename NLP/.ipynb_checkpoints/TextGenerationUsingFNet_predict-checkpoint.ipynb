{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90191508-4e3e-4fab-a879-5da98b533f18",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afa2386-58ec-4359-abec-7960fbc1b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Defining hyperparameters\n",
    "\n",
    "VOCAB_SIZE = 8192\n",
    "MAX_SAMPLES = 50000\n",
    "BUFFER_SIZE = 20000\n",
    "MAX_LENGTH = 40\n",
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49eab16-7838-455e-94ba-f44e119bb57c",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "We will be using the Cornell Dialog Corpus. We will parse the movie conversations into questions and answers sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d14dd9-a7ef-40d5-ae15-0afbbedd5b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 15:13:48.809001: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-19 15:13:48.809103: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = keras.utils.get_file(\n",
    "    \"cornell_movie_dialogs.zip\",\n",
    "    origin=\"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\"\n",
    ")\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, \"movie_lines.txt\")\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset, \"movie_conversations.txt\")\n",
    "\n",
    "\n",
    "def load_conversations():\n",
    "    # Helper function for loading the conversation splits\n",
    "    id2line = {}\n",
    "    with open(path_to_movie_lines, errors=\"ignore\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
    "        id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(path_to_movie_conversations, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
    "        # get conversation in a list of line ID\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(\", \")]\n",
    "        for i in range(len(conversation) - 1):\n",
    "            inputs.append(id2line[conversation[i]])\n",
    "            outputs.append(id2line[conversation[i + 1]])\n",
    "            if len(inputs) >= MAX_SAMPLES:\n",
    "                return inputs, outputs\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "questions, answers = load_conversations()\n",
    "\n",
    "# Splitting training and validation sets\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((questions[:40000], answers[:40000]))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((questions[40000:], answers[40000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64fc2d-86fd-4c67-b08f-9438cc3c117b",
   "metadata": {},
   "source": [
    "## Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3aaeec-f48f-48c9-a366-19e63dbf35e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 15:13:49.059635: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-09-19 15:13:49.059773: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = tf.strings.lower(sentence)\n",
    "    # Adding a space between the punctuation and the last word to allow better tokenization\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"([?.!,])\", r\" \\1 \")\n",
    "    # Replacing multiple continuous spaces with a single space\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"\\s\\s+\", \" \")\n",
    "    # Replacing non english words with spaces\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"[^a-z?.!,]+\", \" \")\n",
    "    sentence = tf.strings.strip(sentence)\n",
    "    sentence = tf.strings.join([\"[start]\", sentence, \"[end]\"], separator=\" \")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    VOCAB_SIZE,\n",
    "    standardize=preprocess_text,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# We will adapt the vectorizer to both the questions and answers\n",
    "# This dataset is batched to parallelize and speed up the process\n",
    "vectorizer.adapt(tf.data.Dataset.from_tensor_slices((questions + answers)).batch(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132dc71-9c15-4885-bc7c-38541c812a7d",
   "metadata": {},
   "source": [
    "## Tokenizing and padding sentences using TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3018b99c-2f6d-469f-a7ae-d57ef4af8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(inputs, outputs):\n",
    "    inputs, outputs = vectorizer(inputs), vectorizer(outputs)\n",
    "    # One extra padding token to the right to match the output shape\n",
    "    outputs = tf.pad(outputs, [[0, 1]])\n",
    "    return (\n",
    "        {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:-1]},\n",
    "        {\"outputs\": outputs[1:]},\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca818edf-206b-489d-8da7-91f58e8d23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
    "        super(FNetEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Casting the inputs to complex64\n",
    "        inp_complex = tf.cast(inputs, tf.complex64)\n",
    "        # Projecting the inputs to the frequency domain using FFT2D and\n",
    "        # extracting the real part of the output\n",
    "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
    "        proj_input = self.layernorm_1(inputs + fft)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1237086e-8d48-4874-bf36-d0dcf878e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class FNetDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(FNetDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
    "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(\n",
    "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
    "    )\n",
    "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
    "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model(\n",
    "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
    "    )\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    fnet = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"fnet\")\n",
    "    return fnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48054323-8dcf-457a-ad8e-b6974116c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet = create_model()\n",
    "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae8ca3c-9a50-4e3d-8622-b40db95f8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"TextGenerationUsingFNet.h5\"\n",
    "callback_list = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "             keras.callbacks.ModelCheckpoint(filename, monitor='accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d010b36-c6f8-4852-b73b-0a6f2f7e3fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.5309 - accuracy: 0.2995"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 15:19:53.908006: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: accuracy improved from -inf to 0.29952, saving model to TextGenerationUsingFNet.h5\n",
      "625/625 [==============================] - 319s 510ms/step - loss: 1.5309 - accuracy: 0.2995 - val_loss: 1.4360 - val_accuracy: 0.3202\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4371 - accuracy: 0.3167\n",
      "Epoch 2: accuracy improved from 0.29952 to 0.31671, saving model to TextGenerationUsingFNet.h5\n",
      "625/625 [==============================] - 313s 501ms/step - loss: 1.4371 - accuracy: 0.3167 - val_loss: 1.4086 - val_accuracy: 0.3283\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.3847 - accuracy: 0.3278\n",
      "Epoch 3: accuracy improved from 0.31671 to 0.32776, saving model to TextGenerationUsingFNet.h5\n",
      "625/625 [==============================] - 314s 502ms/step - loss: 1.3847 - accuracy: 0.3278 - val_loss: 1.3959 - val_accuracy: 0.3317\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.3434 - accuracy: 0.3371\n",
      "Epoch 4: accuracy improved from 0.32776 to 0.33711, saving model to TextGenerationUsingFNet.h5\n",
      "625/625 [==============================] - 307s 490ms/step - loss: 1.3434 - accuracy: 0.3371 - val_loss: 1.3923 - val_accuracy: 0.3346\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.3057 - accuracy: 0.3466\n",
      "Epoch 5: accuracy improved from 0.33711 to 0.34664, saving model to TextGenerationUsingFNet.h5\n",
      "625/625 [==============================] - 303s 486ms/step - loss: 1.3057 - accuracy: 0.3466 - val_loss: 1.3937 - val_accuracy: 0.3336\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.2706 - accuracy: 0.3561\n",
      "Epoch 6: accuracy improved from 0.34664 to 0.35609, saving model to TextGenerationUsingFNet.h5\n",
      "625/625 [==============================] - 317s 508ms/step - loss: 1.2706 - accuracy: 0.3561 - val_loss: 1.4006 - val_accuracy: 0.3336\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.2337 - accuracy: 0.3666\n",
      "Epoch 7: accuracy improved from 0.35609 to 0.36660, saving model to TextGenerationUsingFNet.h5\n",
      "625/625 [==============================] - 320s 512ms/step - loss: 1.2337 - accuracy: 0.3666 - val_loss: 1.4144 - val_accuracy: 0.3318\n",
      "Epoch 7: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x280497250>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnet.fit(train_dataset, epochs=20, validation_data=val_dataset, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ef0c95-1437-4efd-9805-8bd0a9b21ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = vectorizer.get_vocabulary()\n",
    "\n",
    "\n",
    "def decode_sentence(input_sentence):\n",
    "    # Mapping the input sentence to tokens and adding start and end tokens\n",
    "    tokenized_input_sentence = vectorizer(\n",
    "        tf.constant(\"[start] \" + preprocess_text(input_sentence) + \" [end]\")\n",
    "    )\n",
    "    # Initializing the initial sentence consisting of only the start token.\n",
    "    tokenized_target_sentence = tf.expand_dims(VOCAB.index(\"[start]\"), 0)\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # Get the predictions\n",
    "        predictions = fnet.predict(\n",
    "            {\n",
    "                \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
    "                \"decoder_inputs\": tf.expand_dims(\n",
    "                    tf.pad(\n",
    "                        tokenized_target_sentence,\n",
    "                        [[0, MAX_LENGTH - tf.shape(tokenized_target_sentence)[0]]],\n",
    "                    ),\n",
    "                    0,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        # Calculating the token with maximum probability and getting the corresponding word\n",
    "        sampled_token_index = tf.argmax(predictions[0, i, :])\n",
    "        sampled_token = VOCAB[sampled_token_index.numpy()]\n",
    "        # If sampled token is the end token then stop generating and return the sentence\n",
    "        if tf.equal(sampled_token_index, VOCAB.index(\"[end]\")):\n",
    "            break\n",
    "        decoded_sentence += sampled_token + \" \"\n",
    "        tokenized_target_sentence = tf.concat(\n",
    "            [tokenized_target_sentence, [sampled_token_index]], 0\n",
    "        )\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a61824a6-d3e5-4f04-8a4c-b536f320925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 15:51:40.806832: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 389ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m not going to be a [UNK] . '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(\"Where have you been all this time?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd49687c-c90e-4c33-a42a-cc228c9206a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i don t know . '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(\"She okay?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f48ab082-99d3-447c-9374-a0dc5c8f8013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i don t know . '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(\"Like my fear of wearing pastels?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334f01a-8b85-4e36-af93-03a622127b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
